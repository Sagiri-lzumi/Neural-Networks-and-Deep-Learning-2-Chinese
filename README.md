---
description: Readme
---

# 引言

{% hint style="warning" %}
汉化进度
{% endhint %}

* [x] 引言
* [ ] 使用神经网络识别手写数字
  * [ ] 察觉器
  * [ ] 西格玛神经元
  * [ ] 神经网络的结构
  * [ ] 一个简单的网络对手写数字进行分类
  * [ ] 用梯度下降法学习
  * [ ] 部署我们的网络来对数字进行分类
  * [ ] 迈向深度学习
* [ ] 逆传播算法是如何工作的
  * [ ] 热身：基于矩阵的快速神经网络输出的方法
  * [ ] 我们需要关于成本函数的两个假设
  * [ ] 哈达玛的积 s.t
  * [ ] 反向传播的四个基本方程
  * [ ] 四个基本方程的证明（optional）
  * [ ] 反向传播的算法
  * [ ] 反向传播的代码
  * [ ] 在什么意义上反向传播是一种快速的算法
  * [ ] 反向传播：大局观
* [ ] 改进神经网路的学习方式
  * [ ] 交叉熵成本函数
  * [ ] 过度拟合和正则化
  * [ ] 权重初始化
  * [ ] 重新审视手写识别：代码
  * [ ] 如何选择伸进网络的超参数
  * [ ] 其他技术
* [ ] 神经网络可以计算任意函数的直观证明
  * [ ] 两个注意事项
  * [ ] 一个输入和一个输出的普遍性
  * [ ] 许多输入变量
  * [ ] 超越西格玛神经元的扩展
  * [ ] 修复阶梯函数
  * [ ] conclusion
* [ ] 为什么深度神经网路难以训练
  * [ ] 梯度消失的问题
  * [ ] 是什么导致了梯度消失的问题？深度神经网络中的不稳定梯度
  * [ ] 更加复杂的神经网络的中的不稳定梯度
  * [ ] 深度学习的其他障碍
* [ ] 深度学习
  * [ ] 卷积网络的介绍
  * [ ] 卷积网络的实践
  * [ ] 我们的卷积网络的代码
  * [ ] 图像识别的最新进展（2006）
  * [ ] 深度神经网络的其他方法
  * [ ] 关于神经网络的未来
* [ ] 附录：是否有一种简单的智能算法？
* [ ] 常见问题





{% hint style="info" %}
注意：本文仅为Neural Networks and Deep Learning的汉化版本，本人学习之余进行翻译，本人英语程度一般，以下翻译使用机器以及人工校准，仅供学习参考。
{% endhint %}

原书链接：

{% embed url="http://neuralnetworksanddeeplearning.com/" %}
链接
{% endembed %}

原作者：

{% embed url="https://michaelnielsen.org/" %}
链接
{% endembed %}

译者GitHub：

{% embed url="https://github.com/Sagiri-lzumi" %}
链接
{% endembed %}

{% hint style="info" %}
原来看到过关于这一本书汉化，但是它并没有完全做完，很多内容都没有，于是我最终决定利用寒假实践学习机器学习以及对这本书进行汉化。
{% endhint %}

[Books Icon](https://iconscout.com/icons/books) by [The Icon Z](https://iconscout.com/contributors/theiconz)

